# **2. Import and Setup**

import pandas as pd # For data manipulation and analysis
import numpy as np # For numerical operations
import random # For generating random data
import json # For working with JSON data
from typing import TypedDict, List # For type hinting, especially for state management
from faker import Faker # For generating fake data for simulation
from openai import OpenAI # For interacting with OpenAI-compatible LLMs
# from google.colab import userdata # For securely accessing Colab secrets
from datetime import date # For date operations
from langgraph.graph import StateGraph, START, END # For building the LangGraph agent orchestration

# Accessing API key from Colab Secrets (ensure 'OPENAI_API_KEY' is set in your secrets tab)
# openai_api_key = userdata.get('OPENAI_API_KEY')

import os
from openai import OpenAI

openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY not set")

client = OpenAI(
    api_key=openai_api_key,
    base_url="https://api.ai-gateway.tigeranalytics.com"
)


# Initialize the OpenAI client with the API key and custom base URL
client = OpenAI(api_key=openai_api_key, base_url="https://api.ai-gateway.tigeranalytics.com")
# Initialize Faker for generating synthetic data
fake = Faker()
# Define a fixed 'today's date' for consistent data generation
TODAY = date(2026, 1, 24)
# Define the model name to be used for LLM calls
MODEL_NAME = "gemini-2.5-flash"




#3. Define Master State for the System

# Define the master state for the entire system using TypedDict for schema enforcement
class MasterState(TypedDict):
    # Core inputs that can be passed to the graph initially or updated by nodes
    wendys_active: List[str] # List of active Wendy's promotions
    competitor_intel: dict # Detailed metadata for traceability and summary of competitor activities
    customer_insights: str # Summary of customer behavioral insights
    raw_market_signals: pd.DataFrame
    market_trends: List[dict]          # Structured trend signals
    market_context_windows: List[dict] # Timing & situational relevance
    market_trends_summary: str         # Narrative summary for creative agent
    raw_concepts: str # Raw, unstructured offer concepts generated by the creative agent
    # Structured keys to hold synchronized data after processing
    structured_concepts: List[dict] # Refined offer concepts in a structured (JSON) format
    final_report_text: str # Introductory text for the final report
    prioritization_table: pd.DataFrame # DataFrame for visualizing offer prioritization

    # --- Trend â†’ Mechanic Constraints ---
TREND_MECHANIC_MAP = {
    "Gamified Rewards": "challenge, streak, or unlock mechanic",
    "Late-Night Value": "time-boxed or after-hours unlock",
    "Surprise & Delight": "randomized reward trigger",
    "Subscription Meal Bundles": "recurring opt-in benefit",
    "App-Exclusive Perks": "mobile-only gated access"
}

#4. Agent #1 - Building Competitor Intelligence Agent

### 4a. Define the Nodes - Competitor Intelligence Logic

def competitor_analyst_node(state: MasterState):
    """Calculates advanced Threat Scores: Saturation + Velocity + Recency Bias for competitor offers."""

    print("âœ… Competitor Analyst RUNNING")

    # Simulate competitor data generation
    records = []
    for _ in range(500):
        records.append({
            "id": fake.uuid4()[:8],  # Unique ID for traceability
            "brand": random.choice(["McDonald's", "Burger King", "Taco Bell"]),
            "mechanic": random.choice(["BOGO", "Gamified App Challenge", "Loyalty Multiplier"]),
            "obs_date": pd.to_datetime(
                fake.date_between(start_date='-60d', end_date=TODAY)
            )
        })

    df = pd.DataFrame(records)

    # Recency weighting
    df["weight"] = np.exp(
        -0.05 * (pd.Timestamp(TODAY) - df["obs_date"]).dt.days
    )

    # Identify competitor mechanics Wendy's is NOT active in
    gaps = df[~df["mechanic"].isin(state["wendys_active"])]

    threats = []

    # âœ… FIXED INDENTATION STARTS HERE
    for mech, group in gaps.groupby("mechanic"):

        # Leading competitor brand
        top_brand = (
            group.groupby("brand")["weight"].sum().idxmax()
        )

        # Raw score + HARD CAP
        raw_score = group["weight"].sum()
        score = round(min(10.0, raw_score), 1)  # ðŸ”’ NEVER > 10

        # Traceability ID
        trace_id = (
            group.sort_values("weight", ascending=False)["id"].iloc[0]
        )

        threats.append(
            f"{mech} driven by {top_brand} "
            f"(Threat: {score}/10) [Ref ID: {trace_id}]"
        )

    return {
        "competitor_intel": {
            "summary": "\n".join(threats),
            "raw": gaps.to_dict()
        }
    }

    # Return the summary of threats and the raw data for potential further analysis
    return {"competitor_intel": {"summary": "\n".join(threats), "raw": gaps.to_dict()}}

#5. Agent #2 - Building Customer Insights Agent

### 5a. Define the Nodes - Customer Insights Logic

from faker import Faker
import random
import json
from collections import Counter

fake = Faker()
random.seed(42)  # Deterministic output

def customer_analyst_node(state: "MasterState"):
    """Analyzes behavioral signals to identify high-redemption segments and customer preferences."""

    print("âœ… Customer Analyst RUNNING")

    """
    Customer Insight Agent (Hybrid, Hallucination-Safe)

    Step 1: Generate synthetic data
    Step 2: Compute metrics deterministically (no LLM)
    Step 3: Ask LLM ONLY to summarize provided metrics
    """

    # ---------------------------------------------------
    # 1. SYNTHETIC SIGNAL GENERATION (NEUTRAL)
    # ---------------------------------------------------
    sample_size = 100
    logs = []

    for _ in range(sample_size):
        logs.append({
            "customer_type": random.choice(
                ["Loyalty Member", "Guest", "First-Timer"]
            ),
            "channel": random.choice(
                ["Mobile App", "Drive-Thru", "In-Store"]
            ),
            "coupon_used": random.choice([True, False])
        })

    # ---------------------------------------------------
    # 2. METRIC COMPUTATION (SOURCE OF TRUTH)
    # ---------------------------------------------------
    redeemed_logs = [l for l in logs if l["coupon_used"]]

    if not redeemed_logs:
        return {
            "customer_insights": "No coupon redemptions observed in the synthetic sample."
        }

    total_redemptions = len(redeemed_logs)

    segment_counts = Counter(l["customer_type"] for l in redeemed_logs)
    channel_counts = Counter(l["channel"] for l in redeemed_logs)

    metrics = {
        "total_redemptions": total_redemptions,
        "segment_share": {
            seg: round(cnt / total_redemptions, 3)
            for seg, cnt in segment_counts.items()
        },
        "channel_share": {
            ch: round(cnt / total_redemptions, 3)
            for ch, cnt in channel_counts.items()
        }
    }

    # ---------------------------------------------------
    # 3. LLM NARRATION (STRICTLY BOUND)
    # ---------------------------------------------------
    analysis_prompt = f"""
You are a Customer Insights Analyst.

IMPORTANT RULES:
- Use ONLY the metrics provided below.
- Do NOT infer causes, preferences, or intent.
- Do NOT introduce new numbers or segments.
- Use descriptive language only (e.g., "accounts for", "observed share").

Metrics (synthetic data):
{json.dumps(metrics, indent=2)}

Write 3 concise bullet-point insights suitable for an executive summary.
"""

    res = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": analysis_prompt}],
        temperature=0  # Minimizes creativity
    )

    # ---------------------------------------------------
    # 4. AGENT OUTPUT
    # ---------------------------------------------------
    return {
        "customer_insights": res.choices[0].message.content,
        "metrics": metrics,
        "data_disclaimer": (
            "Insights are based on synthetic data and "
            "are descriptive only."
        )
    }

#6. Agent #3 - Building Market Trends Agent

### 6a. Define the Nodes - Market Trends Logic

# **NEW NODE A â€” Market Context Signal Generator**

def market_context_generator(state: MasterState):
    records = []

    trend_types = [
        "Gamified Rewards",
        "Subscription Meal Bundles",
        "Surprise & Delight",
        "Late-Night Value",
        "App-Exclusive Perks"
    ]

    seasons = ["Winter", "Spring", "Summer", "Fall"]
    dayparts = ["Breakfast", "Lunch", "Dinner", "Late Night"]
    situations = ["Cold Weather", "Payday", "Commute", "Weekend"]
    sources = ["Reddit", "TikTok", "Press", "Food Blogs"]

    for _ in range(900):
        records.append({
            "trend_type": random.choice(trend_types),
            "season": random.choice(seasons),
            "daypart": random.choice(dayparts),
            "situation": random.choice(situations),
            "source": random.choice(sources),
            "observed_date": fake.date_between(start_date='-120d', end_date=TODAY)
        })

    return {"raw_market_signals": pd.DataFrame(records)}


# **Market Context Analyst (Structured Scoring)**

def market_context_analyst(state: MasterState):
    df = state["raw_market_signals"].copy()

    # Normalize dates
    df["observed_date"] = pd.to_datetime(df["observed_date"])
    df["days_ago"] = (pd.Timestamp(TODAY) - df["observed_date"]).dt.days

    # Recency decay weighting
    df["weight"] = np.exp(-0.05 * df["days_ago"])

    windows = []

    # Aggregate timing/context signals
    for keys, group in df.groupby(
        ["trend_type", "season", "daypart", "situation"]
    ):
        strength = group["weight"].sum()
        ref_id = group.index[0]  # lightweight traceability ID

        windows.append({
            "signal_id": f"CTX-{ref_id}",
            "trend": keys[0],
            "season": keys[1],
            "daypart": keys[2],
            "situation": keys[3],
            "timing_strength": round(strength, 2)
        })

    # Guardrail (prevents divide-by-zero)
    if not windows:
        return {"market_context_windows": []}

    max_strength = max(w["timing_strength"] for w in windows)

    # Score + classify
    for w in windows:
        w["relevance_score"] = round(
            (w["timing_strength"] / max_strength) * 10, 1
        )
        w["confidence"] = round(
            min(1.0, w["timing_strength"] / (0.75 * max_strength)), 2
        )
        w["action"] = (
            "Act Now" if w["relevance_score"] >= 7
            else "Monitor"
        )

    # âœ… NEW: Cap to Top 5 Most Relevant Windows
    windows = sorted(
        windows,
        key=lambda x: x["relevance_score"],
        reverse=True
    )[:5]

    return {
        "market_context_windows": windows
    }


# **Convert Structured Context â†’ Narrative Trends**

def market_trends_narrator(state: MasterState):
    context = json.dumps(state["market_context_windows"], indent=2)

    print("âœ… Market Trends Narrator RUNNING")

    prompt = f"""
    You are Wendyâ€™s Market Strategist.

    Using the timing and context signals below:
    - Summarize the top emerging trends
    - Explain WHEN and WHY they matter
    - Use consumer language and value cues

    DATA:
    {context}
    """

    res = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}]
    )

    return {"market_trends_summary": res.choices[0].message.content}


def context_ready_gate(state: MasterState):
    """
    Synchronization node.
    Ensures all required upstream context is present
    before offer design runs.
    """
    return {}


#7. Agent #4 - Building Offer Design Orchestrator

### 7a. Creative Designer (Creativity & Differentiation)

def offer_designer_node(state: MasterState):

    print("âœ… Offer Designer RUNNING")

    assert "market_trends_summary" in state, \
        f"Missing market_trends_summary. Available keys: {list(state.keys())}"

    prompt = f"""
You are Wendy's Lead Creative Strategist.

Use the following intelligence signals to design differentiated offers.

COMPETITOR GAPS:
{state['competitor_intel']['summary']}

CUSTOMER INSIGHTS:
{state['customer_insights']}

TRENDS & CONTEXT:
{state['market_trends_summary']}

TOP SIGNALS (with evidence IDs):
{json.dumps(state["market_context_windows"], indent=2)}

MECHANIC CONSTRAINTS BY TREND:
{json.dumps(TREND_MECHANIC_MAP, indent=2)}

Constraints:
- Each offer MUST explicitly reference at least one signal_id
- The mechanic MUST align with the associated trend type
- Do NOT reuse standard discount formats (e.g., % off, $ off)

Task:
Design 2 original Wendy's offers.

Rules:
- One offer must be a DEFENSIVE response
- One offer must be a FIRST-TO-MARKET creative pivot

OUTPUT FORMAT (STRICT):
Name:
Why:
Strategy: Defensive | First-to-Market
Evidence Signal IDs:

Example:
Evidence Signal IDs: CTX-12, CTX-44
"""

    res = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}]
    )

    return {"raw_concepts": res.choices[0].message.content}


### 7b. Critique Loop (Agent Collaboration)

def brand_validator_node(state: MasterState):
    """Refines raw concepts with Wendy's brand voice and returns them in a structured JSON format."""
    # Construct a prompt to refine concepts and enforce a specific JSON output format
    prompt = f"""
    Refine these concepts with Wendy's witty brand voice: {state['raw_concepts']}

    RETURN ONLY A JSON OBJECT in this format:
    {{
      "report_intro": "Intro text here...",
      "offers": [
        {{
          "name": "The Wendy's Daily Drip Deal",
          "witty_rationale": "Rationale text...",
          "type": "Defensive",
          "evidence_signals": ["CTX-12"],
          "feasibility": 9.0,
          "impact": 8.5
        }},
        ...
      ]
    }}
    """
    # Call the LLM, specifying JSON object as the desired response format
    res = client.chat.completions.create(
        model=MODEL_NAME,
        response_format={ "type": "json_object" }, # Ensures valid JSON output
        messages=[{"role": "user", "content": prompt}]
    )

    # Parse the JSON string from the LLM response
    data = json.loads(res.choices[0].message.content)
    # Update the state with structured concepts and the report introduction
    return {
        "structured_concepts": data["offers"],
        "final_report_text": data["report_intro"]
    }

### 7c. Visualization Scorecard

def visualization_node(state: MasterState):
    """Dynamically builds a prioritization table using the EXACT names and scores from the Validator Agent's output."""
    # Retrieve the structured concepts from the current state
    concepts = state["structured_concepts"]

    table_data = []
    # Iterate through each structured concept to prepare data for the DataFrame
    for item in concepts:
        table_data.append({
            "Concept": item["name"], # Use the refined concept name
            "Feasibility": item["feasibility"],
            "Impact": item["impact"],
            "Confidence": round((item["impact"] + item["feasibility"]) / 20, 2),
            "Type": item["type"]
        })

    # Create a Pandas DataFrame from the prepared data
    return {"prioritization_table": pd.DataFrame(table_data)}

#8. Build and Compile the Graph

### 8a. Build the Graph Architecture

builder = StateGraph(MasterState) # Initialize the graph with the defined MasterState schema

builder.add_node("ready", context_ready_gate)


# Add all agent nodes to the graph
builder.add_node("comp", competitor_analyst_node) # Node for Competitor Intelligence
builder.add_node("cust", customer_analyst_node) # Node for Customer Insights
builder.add_node("mkt_gen", market_context_generator)
builder.add_node("mkt_ctx", market_context_analyst)
builder.add_node("trend", market_trends_narrator)
builder.add_node("design", offer_designer_node) # Node for Offer Design (Creative Strategist)
builder.add_node("validate", brand_validator_node) # Node for Brand Validation and Structuring
builder.add_node("viz", visualization_node) # Node for Visualization/Prioritization Table

### 8b. Orchestration Flow

# Step 1: Define edges for parallel execution of initial analytical agents
# All three analytical agents start their work concurrently from the initial state.
# Parallel starts
builder.add_edge(START, "comp")
builder.add_edge(START, "cust")
builder.add_edge(START, "mkt_gen")

# Market context chain
builder.add_edge("mkt_gen", "mkt_ctx")
builder.add_edge("mkt_ctx", "trend")


# Step 2: Define edges for the orchestrator (Offer Designer) and subsequent validation
# All three analytical agents' outputs feed into the Offer Designer.
# All intelligence must finish first
# Design must wait for trend (hard dependency)
builder.add_edge("trend", "design")

# Downstream
builder.add_edge("design", "validate")
builder.add_edge("validate", "viz")
builder.add_edge("viz", END)


# Step 3: Link the Brand Validator to the Visualizer and then to the end of the graph
# The structured concepts from the Validator are used to create the visualization.
builder.add_edge("validate", "viz")
# The graph concludes after the visualization is prepared.
builder.add_edge("viz", END)

#9. Compile the Final App

app = builder.compile()
